import numpy as np
import scipy.io as sio
import scipy.linalg as sl
import sklearn.linear_model as slm
import matplotlib.pyplot as plt

'''
thibaux.mat was generated by matlab codes, exactly the same as in book
'''

# 这个是matlab里面的方法， 其实与sklearn.Ridge是一样的
def GetW(X, y, L, addOnes=True):
    N, D = X.shape
    if addOnes:
        X_train = np.c_[np.ones(N), X]
    else:
        X_train = X
    N, D = X_train.shape
    lambdaVec = L * np.ones(D)
    if addOnes:
        lambdaVec[0] = 0  # don't penalize the bias term
    LMat = np.diag(lambdaVec)**0.5  # D * D
    XX = np.r_[X_train, LMat]       # (N + D) * D
    yy = np.r_[y.reshape(-1, 1), np.zeros((D, 1))] # (N + D) * 1
    
    return np.dot(sl.inv(np.dot(XX.T, XX)), np.dot(XX.T, yy))

def kernelise(X, centers, sigma):
    N, D = X.shape
    result = np.zeros((N, len(centers)))
    for i in range(len(centers)):
        center = centers[i]
        xi = X - center
        norm = sl.norm(xi, ord=2, axis=1)**2
        result[:, i] = np.exp(-norm / (2 * sigma**2))

    coef = 1 / (2 * np.pi * sigma**2)**0.5
    return result * coef

def plot(i, xtrain, ytrain, xtest, centers, sigma):
    subplotindices = np.array([1, 2, 3]) + i * 3
    # 在fit model时，basis使用的是每个数据点，这个一定要注意
    X = kernelise(xtrain, xtrain.ravel(), sigma)   
    Xtest = kernelise(xtest, xtrain.ravel(), sigma)
    alpha = 0.001   # very small l2 regularization, just for numerical stability
    ridge = slm.Ridge(alpha=alpha, fit_intercept=True).fit(X, ytrain)
    #ols = slm.LinearRegression(fit_intercept=True).fit(X, ytrain)
    #print(ols.coef_, ols.intercept_)
    print(ridge.coef_, ridge.intercept_)

    y_test_predict = ridge.predict(Xtest)

    # plot regression
    indexStr = '33' + str(subplotindices[0])
    plt.subplot((int)(indexStr))
    plt.axis([0, 20, -10, 20])
    plt.xticks(np.linspace(0, 20, 5))
    plt.yticks(np.linspace(-10, 20, 4))
    plt.plot(xtrain, ytrain, color='darkblue', marker='o', ms=10, linestyle='none')
    plt.plot(xtest, y_test_predict, 'k-', lw=2)

    # plot kernels
    indexStr = '33' + str(subplotindices[1])
    plt.subplot((int)(indexStr))
    plt.xlim(0, 20)
    plt.xticks(np.linspace(0, 20, 5))
    Xtest2 = kernelise(xtest, centers, sigma)
    for i in range(len(centers)):
        plt.plot(xtest, Xtest2[:, i], color='midnightblue')

    # plot design matrix
    indexStr = '33' + str(subplotindices[2])
    plt.subplot((int)(indexStr))
    Xtrain2 = kernelise(xtrain, centers, sigma)
    plt.imshow(Xtrain2, cmap='gray', interpolation='none', aspect='auto', extent=[0, 10, 20, 0])

# prepare data
data = sio.loadmat('thibaux.mat')
print(data.keys())
xtrain, ytrain = data['xtrain'], data['ytrain']
xtest, ytest = data['xtest'], data['ytest']
print(xtrain.shape, xtest.shape)
print(xtrain.dtype, xtest.dtype)

K = 10
centers = np.linspace(np.min(xtrain), np.max(xtrain), K)
sigmas = [0.5, 10, 50] # correspond to scale of RBF kernel
xtrain = xtrain.reshape(-1, 1).astype('float64')  # xtrain的dtype是unit8.. 这样的话用0 - 1就会溢出，变成255..
xtest = xtest.reshape(-1, 1).astype('float64')

# plots
fig = plt.figure(figsize=(12, 8))
fig.canvas.set_window_title('linregRbfDemo')

for i in range(len(sigmas)):
    sigma = sigmas[i]
    plot(i, xtrain, ytrain, xtest, centers, sigma)

plt.tight_layout()
plt.show()
