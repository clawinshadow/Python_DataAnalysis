import numpy as np
import sklearn.tree as st

'''
决策树的中心思想在于每次划分都要最大程度上提高数据集的纯度(purity)，如何衡量数据集的纯度主要有三种方法：
令 数据集为D, |D|为数据集的总行数，|D(j)|为数据集按某个属性分类后各分类的行数，所有的对数都是以2为底的

1. 信息增益：使用香农提出的信息熵来度量一个数据集的纯度，这个算法是最古老的决策树算法称之为 ID3
             数据集的信息熵：Info(D) = -(p1*log(p1) + p2*log(p2) +...+pn*log(pn))
             pi是D中任意元素属于类Ci的概率，用|C(i, D)| / |D|来估计,
             根据属性A划分后的信息熵：
                               |D(j)|
             Info(A, D) = Sum(-------- * Info(D(j)) ), j就是A中不同元素的数量
                                |D|

             那么划分后的信息增益 Gain(A) = Info(D) - Info(A, D), 信息熵越小，纯度越高。
             我们尝试着用每个属性去划分数据集D，计算每个Gain(A(i))，然后选取最大的那个属性来划分D
             
2. 信息增益率：ID3算法偏向于选择分类数量最多的属性，假设采用数据集的行编号来划分数据集，因为每个编号都不一样，
             一次划分后就达到了最优解，但根据行编号来划分显然是没意义的。所以后人提出了另一种方法来改善这个问题
             它用分裂信息(split information)来将信息增益规范化，类似于Info(D):
                                   |D(j)|        |D(j)|
             SplitInfo(A, D) = -Sum( ------- * log(-------))
                                    |D|           |D|

                                   Gain(A, D)
             GainRate(A, D) = ---------------------
                                 SplitInfo(A, D)
                                 
             GainRate就是信息增益率，该算法称之为C4.5

3. 基尼指数：Gini Index, 基尼指数度量数据集的不纯度，该算法称之为CART

             Gini(D) = 1 - Sum(p(i)**2), pi是D中任意元素属于类Ci的概率，用|C(i, D)| / |D|来估计, 与ID3一致

             与前面两个不同的是，当用属性A划分D时，这个划分规则变得非常不一样，基尼指数考虑每个属性的二元划分
             假如A：{1, 2, 3}, 那么A的二元划分就是形如 {1, 2} - {3}这种，如果A有v个可能的取值，
             那么A就有2**v个可能的子集，不考虑满集和空集，存在(2**v - 2) / 2 种不同的划分，计算每种可能的划分：
                             |D(1)|                |D(2)|
             Gini(A(i), D) = ------ * Gini(D(1)) + ------ * Gini(D(2))
                               |D|                  |D|

             Delta(Gini(A)) = Gini(D) - Gini(A(i), D),
             选取最大化这个值的二元划分就好

关于这三种方法的综合评定，ID3倾向于选择多值属性，尽管增益率调整了这种便宜，但是它倾向于产生不平衡的划分，其中
一个分区比其他分区小得多。基尼指数偏向于多值属性，并且当类的数量很大时会有计算上的困难，因为这个是指数级的复杂度，
并且它还倾向于导致相等大小的分区和纯度。anyway，尽管这些算法都是有偏的，但是这些度量在实际中都产生了相当好的结果


             
'''



